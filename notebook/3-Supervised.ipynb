{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload \n",
    "%autoreload 2\n",
    "\n",
    "# Math Stuff\n",
    "import ee, scipy.misc, random, os\n",
    "import numpy as np\n",
    "from threading import Thread\n",
    "\n",
    "# GEE stuff\n",
    "from gee_library import *\n",
    "ee.Initialize()\n",
    "\n",
    "# debug stuff\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "def tile_at(center_long_lat, meters, pixels, bands):\n",
    "    \"\"\" Returns a (pixels, pixels, len(bands)) dimention numpy array of a \n",
    "    meters by meters section of earth centered at center_long_lat.\"\"\"\n",
    "    # Calculate resolution\n",
    "    resolution = meters/pixels\n",
    "    \n",
    "    # Get GPS box\n",
    "    tile_bounds = square_centered_at(\n",
    "        point = center_long_lat,\n",
    "        half_distance = meters\n",
    "    )\n",
    "    \n",
    "    # load map\n",
    "    monterey_collection = ee.ImageCollection('USDA/NAIP/DOQQ')\\\n",
    "        .filterBounds(tile_bounds)\n",
    "#         .filter(ee.Filter.date('2016-01-01', '2017-01-01'))\n",
    "        \n",
    "    # request imagery\n",
    "    tiles = img_at_region(monterey_collection, resolution, bands, tile_bounds)\n",
    "    # resize img to requested size\n",
    "    np_band_array = [scipy.misc.imresize(tiles[b], (pixels, pixels)) for b in bands]\n",
    "    # and stack the images in a matrix\n",
    "    return np.dstack(np_band_array)\n",
    "\n",
    "\n",
    "def random_tile_within(coords, meters, pixels, bands):\n",
    "    \"\"\" Chooses a random tile within the bounds of the coords.\n",
    "    This function finds a random coordinate within the bounds defined by\n",
    "    coords, then calls `tileat()` to grab that tile.\n",
    "    \n",
    "    `coords`: ((longmin, latmin),(longmax, latmax))\n",
    "    \n",
    "    Hits the server twice :/\"\"\"\n",
    "    \n",
    "    ((longmin, latmin),(longmax, latmax)) = coords\n",
    "    \n",
    "    # get random coords\n",
    "    longitude = random.uniform(longmin, longmax)\n",
    "    latitude = random.uniform(latmin, latmax)\n",
    "    \n",
    "    return tile_at((longitude, latitude), meters, pixels, bands)\n",
    "\n",
    "def save_random_tile_at(coords, meters, pixels, bands, file_name):\n",
    "    img = random_tile_within(coords, meters, pixels, bands)\n",
    "    scipy.misc.toimage(img, cmin=0.0, cmax=-1).save(file_name)\n",
    "    return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from threading import Thread\n",
    "\n",
    "\n",
    "# Define the bounds of each geographical area\n",
    "# Cities\n",
    "chicago = ((-87.769242, 41.663568), (-87.631913, 41.993581))\n",
    "newyork = ((-73.995267, 40.596924), (-73.905278, 40.693623))\n",
    "atlanta = ((-84.409626, 33.721386), (-84.356754, 33.774483))\n",
    "sanfransisco = ((-122.448245, 37.740577), (-122.381984, 37.801367))\n",
    "# Farmland\n",
    "kansas = ((-99.647878, 38.457433), (-98.840109, 39.464683))\n",
    "montana = ((-108.323227, 46.422764), (-106.332494, 47.224012))\n",
    "georgia = ((-82.783325, 31.581322), (-82.492991, 31.813600))\n",
    "# Mountains\n",
    "sierranevadas = ((-122.533712, 43.170967), (-122.302144, 44.679019))\n",
    "cascades = ((-121.640730, 47.486787), (-120.682082, 48.886281))\n",
    "rockies = ((-106.577972, 38.700841), (-105.828700, 39.798840))\n",
    "\n",
    "training_cities = [sanfransisco, atlanta, newyork]\n",
    "test_city = [chicago]\n",
    "training_mountains = [sierranevadas, rockies]\n",
    "test_mountains=[cascades]\n",
    "training_farms=[kansas, montana]\n",
    "test_farms=[georgia]\n",
    "\n",
    "\n",
    "# Define where we will save the data\n",
    "DATA_DIR=\"./ch3_data\"\n",
    "TRAIN_IMG_DIR=os.path.join(DATA_DIR, \"train_imgs\")\n",
    "TRAIN_IMG_LABELS=os.path.join(DATA_DIR, \"train.txt\")\n",
    "TEST_IMG_DIR=os.path.join(DATA_DIR, \"test_imgs\")\n",
    "TEST_IMG_LABELS=os.path.join(DATA_DIR, \"test.txt\")\n",
    "\n",
    "# make directories if they don't exist\n",
    "for d in [DATA_DIR, TRAIN_IMG_DIR, TEST_IMG_DIR]:\n",
    "    if not os.path.exists(d):\n",
    "        os.makedirs(d)\n",
    "\n",
    "\n",
    "def download_data(gps_bound_list, number_of_examples, directory, delay=2.5):\n",
    "    \"\"\" Downloads random tiles from a list of regions to `directory`.\n",
    "    Spawns a thread for each image with a delay of `delay` seconds\n",
    "    between thread spawns.\n",
    "    \n",
    "    Since each tile hits the server twice, and Google limits api requests\n",
    "    to 3 per second, any value less than 1.5 will likely result in server\n",
    "    errors.\"\"\"\n",
    "    \n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    \n",
    "    for i in range(number_of_examples):\n",
    "        if i%10 == 0:\n",
    "            print i\n",
    "        t = Thread(target=save_random_tile_at,\n",
    "                   args=(random.choice(gps_bound_list),\n",
    "                         100,\n",
    "                         50,\n",
    "                         ['R', 'G', 'B'],\n",
    "                         os.path.join(directory, str(i)+'.png')))\n",
    "        t.start()\n",
    "        time.sleep(delay)\n",
    "    \n",
    "\n",
    "# #\n",
    "# # Download training data\n",
    "# #\n",
    "# number_per_training_class = 1000\n",
    "# print 'Grabbing training farms...'\n",
    "# download_data(training_farms, number_per_training_class, os.path.join(TRAIN_IMG_DIR,'farms'))\n",
    "# print 'Grabbing training cities...'\n",
    "# download_data(training_cities, number_per_training_class, os.path.join(TRAIN_IMG_DIR,'cities'))\n",
    "# print 'Grabbing training mountains...'\n",
    "# download_data(training_mountains, number_per_training_class, os.path.join(TRAIN_IMG_DIR,'mountains'))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Populate label files as comma-separated values (CSV)\n",
    "#\n",
    "\n",
    "\n",
    "with open(TRAIN_IMG_LABELS, \"w\") as myfile:\n",
    "    klass = 0\n",
    "    path = os.path.join(TRAIN_IMG_DIR,'farms')\n",
    "    for f in os.listdir(path):\n",
    "        filename = os.path.join(path, f)\n",
    "        myfile.write(filename + \",\" + str(klass) + \"\\n\")\n",
    "        \n",
    "        \n",
    "    klass = 1\n",
    "    path = os.path.join(TRAIN_IMG_DIR,'cities')\n",
    "    for f in os.listdir(path):\n",
    "        filename = os.path.join(path, f)\n",
    "        myfile.write(filename + \",\" + str(klass) + \"\\n\")\n",
    "        \n",
    "    klass = 2\n",
    "    path = os.path.join(TRAIN_IMG_DIR,'mountains')\n",
    "    for f in os.listdir(path):\n",
    "        filename = os.path.join(path, f)\n",
    "        myfile.write(filename + \",\" + str(klass) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "#\n",
    "# Create Data Reader\n",
    "#\n",
    "\n",
    "# Create a string-producer tensor to actually input the filename into the system\n",
    "filename_queue = tf.train.string_input_producer([TRAIN_IMG_LABELS])\n",
    "\n",
    "# Connect it to a line-reader tensor\n",
    "reader = tf.TextLineReader()\n",
    "key, value = reader.read(filename_queue)\n",
    "\n",
    "# Connect that raw line-reader to a csv decoder\n",
    "filename, labels = tf.decode_csv(value, record_defaults=[[\"error_filename\"], [\"error_label\"]])\n",
    "\n",
    "# Since the CSV has filenames in it, direct those filenames to another filereader (this time reads\n",
    "# the whole file since they're png files)\n",
    "file_contents = tf.read_file(filename)\n",
    "\n",
    "# connect that file reader to a png decoder\n",
    "png_data = tf.image.decode_png(file_contents, channels=3)\n",
    "png_data_resized = tf.image.resize_images(png_data, (50, 50)) # Because we need to define an explicit size\n",
    "\n",
    "# Since pngs are 8-bit integers, let's convert them to floats for the NN\n",
    "features = tf.cast(png_data_resized, tf.float32)\n",
    "\n",
    "# Now `features` is an image queue, and `label` is a label queue!\n",
    "\n",
    "# Once you start training, don't forget to start up the queue!\n",
    "# coord = tf.train.Coordinator()\n",
    "# threads = tf.train.start_queue_runners(coord=coord)\n",
    "# https://www.tensorflow.org/programmers_guide/reading_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import misc\n",
    "\n",
    "def gee_batch(batch_size):\n",
    "    \"\"\"Generates batches of size `batch_size`\"\"\"\n",
    "    \n",
    "    # read file\n",
    "    lines = open(TRAIN_IMG_LABELS).read().splitlines()\n",
    "    \n",
    "    img_list = []\n",
    "    label_list = []\n",
    "    for i in range(batch_size):\n",
    "        png_path, label = random.choice(lines).split(',')\n",
    "        img_list.append(misc.imread(png_path).flatten())\n",
    "#         img_list.append(np.zeros(50*50*3))\n",
    "        one_hot = np.zeros(3)\n",
    "        one_hot[int(label)] = 1\n",
    "        label_list.append(one_hot)\n",
    "        \n",
    "    return np.stack(img_list, axis=0), np.stack(label_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Model\n",
    "#\n",
    "\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 50*50*3])\n",
    "y_ = tf.placeholder(tf.float32, [None, 3])\n",
    "\n",
    "\n",
    "\n",
    "# Ensure our images are the correct shape\n",
    "input_layer = tf.reshape(x, [-1, 50, 50, 3])\n",
    "\n",
    "\n",
    "#\n",
    "# Convolutions\n",
    "#\n",
    "\n",
    "# Convolutional Layer #1\n",
    "conv1 = tf.layers.conv2d(\n",
    "    inputs=input_layer,\n",
    "    filters=32,\n",
    "    kernel_size=[3, 3],\n",
    "    padding=\"valid\",\n",
    "    activation=tf.nn.relu)\n",
    "\n",
    "# Pooling Layer #1\n",
    "pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
    "\n",
    "# Convolutional Layer #2 and Pooling Layer #2\n",
    "conv2 = tf.layers.conv2d(\n",
    "    inputs=pool1,\n",
    "    filters=64,\n",
    "    kernel_size=[3, 3],\n",
    "    padding=\"valid\",\n",
    "    activation=tf.nn.relu)\n",
    "pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
    "\n",
    "# Convolutional Layer #3 (no pooling)\n",
    "conv3 = tf.layers.conv2d(\n",
    "    inputs=pool2,\n",
    "    filters=64,\n",
    "    kernel_size=[3, 3],\n",
    "    padding=\"valid\",\n",
    "    activation=tf.nn.relu)\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "# Fully Connected\n",
    "#\n",
    "\n",
    "\n",
    "# Dense Layer\n",
    "conv3_flat = tf.reshape(conv3, [-1, 9 * 9 * 64])\n",
    "dense = tf.layers.dense(inputs=conv3_flat, units=1024, activation=tf.nn.relu)\n",
    "dropout = tf.layers.dropout(\n",
    "    inputs=dense,\n",
    "    rate=0.4,\n",
    "    training= True)\n",
    "\n",
    "# Logits Layer\n",
    "logits = tf.layers.dense(inputs=dropout, units=3)\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "# Loss\n",
    "#\n",
    "\n",
    "# Turn our numerical labels into one-hot arrays\n",
    "# onehot_labels = tf.one_hot(indices=tf.cast(y_, tf.int32), depth=3)\n",
    "onehot_labels = y_\n",
    "\n",
    "loss = None\n",
    "train_op = None\n",
    "\n",
    "loss = tf.losses.softmax_cross_entropy(\n",
    "    onehot_labels=onehot_labels, # ground truth\n",
    "    logits=logits) # network output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0\n",
      "step 100, training accuracy 1\n",
      "step 200, training accuracy 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-43f1101f53a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m# run an iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     optimizer.run(feed_dict={x:eb,\n\u001b[0;32m---> 39\u001b[0;31m                              y_: lb})\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python2/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m   1586\u001b[0m         \u001b[0mnone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0msession\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mused\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1587\u001b[0m     \"\"\"\n\u001b[0;32m-> 1588\u001b[0;31m     \u001b[0m_run_using_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1589\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python2/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36m_run_using_default_session\u001b[0;34m(operation, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   3830\u001b[0m                        \u001b[0;34m\"the operation's graph is different from the session's \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3831\u001b[0m                        \"graph.\")\n\u001b[0;32m-> 3832\u001b[0;31m   \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/opt/conda/envs/python2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#\n",
    "# Train\n",
    "# \n",
    "\n",
    "# # optimizer\n",
    "# optimizer = tf.contrib.layers.optimize_loss(\n",
    "#     loss=loss,\n",
    "#     global_step=tf.contrib.framework.get_global_step(),\n",
    "#     learning_rate=0.001,\n",
    "#     optimizer=\"SGD\")\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(logits,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# Initialize tensorflow\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "# And run 20k iterations\n",
    "for i in range(20000):\n",
    "    # make batch\n",
    "    example_batch, label_batch = gee_batch(1)\n",
    "    \n",
    "    \n",
    "    # Debug output\n",
    "    if i%100 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={x:example_batch,\n",
    "                                                 y_: label_batch})\n",
    "        print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "        \n",
    "    # run an iteration\n",
    "    optimizer.run(feed_dict={x:example_batch,\n",
    "                             y_: label_batch})\n",
    "\n",
    "    \n",
    "# Final accuracy\n",
    "print(\"test accuracy %g\"%accuracy.eval(feed_dict={\n",
    "    x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n8SH/D7iHJEOi6f6DAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ffba84b4610>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-5.35693073  7.40625763 -3.61006737]]\n"
     ]
    }
   ],
   "source": [
    "# Prediction\n",
    "\n",
    "# farm, city, mountain\n",
    "\n",
    "example_batch, label_batch = gee_batch(1)\n",
    "   \n",
    "\n",
    "predictions = sess.run(logits, feed_dict={x:example_batch, y_: label_batch})\n",
    "\n",
    "img = example_batch.reshape((50,50,3))\n",
    "plt.imshow(img); plt.show()\n",
    "\n",
    "print \"predictions:\", predictions\n",
    "print \"ground truth:\", label_batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BADLANDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# initialize interactive session\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# inputs ('None' refers to batch size)\n",
    "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "\n",
    "#\n",
    "# Helper functions for creating layers and initializing variables\n",
    "#\n",
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##############\n",
    "# First Layer\n",
    "#\n",
    "\n",
    "# make variables (5x5 kernel, 1 channel img, 32 filters)\n",
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "\n",
    "# reshape input (-1 is batch size)\n",
    "x_image = tf.reshape(x, [-1,28,28,1])\n",
    "\n",
    "# conv and pooling\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "\n",
    "\n",
    "##############\n",
    "# Second Layer\n",
    "#\n",
    "\n",
    "# make variables (32 channels coming in, 64 going out)\n",
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "# no reshaping of input necessary. input h_pool1 should have shape ...\n",
    "\n",
    "# conv and pooling\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#######################\n",
    "# Fully Connected Layer\n",
    "#\n",
    "\n",
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "# flatten convolution layer for FC operations\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "\n",
    "###########\n",
    "# Dropout\n",
    "#\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32) # allows us to turn DO on and off during training and testing\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "\n",
    "################\n",
    "# Readout layer\n",
    "#\n",
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Traning and testing\n",
    "#\n",
    "\n",
    "# loss\n",
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "\n",
    "# optimizer\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# Initialize tensorflow\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# And run 20k iterations\n",
    "for i in range(20000):\n",
    "    \n",
    "    # make batch\n",
    "    batch = mnist.train.next_batch(50)\n",
    "    \n",
    "    # Debug output\n",
    "    if i%100 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={\n",
    "            x:batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "        print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "        \n",
    "    # run an interation\n",
    "    train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "\n",
    "    \n",
    "# Final accuracy\n",
    "print(\"test accuracy %g\"%accuracy.eval(feed_dict={\n",
    "    x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Mnist version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "#\n",
    "# Define Network\n",
    "#\n",
    "\n",
    "# A placeholder is a thing that *we* will be filling\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "\n",
    "# A variable is a thing that we can fill and the network can also change\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# prediction\n",
    "y = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "\n",
    "# ground truth\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# loss function\n",
    "# cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y)\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n",
    "\n",
    "#\n",
    "# Train\n",
    "#\n",
    "\n",
    "# Define optimizer\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "\n",
    "# initialize session\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# initialize variables\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "# train 1000 steps\n",
    "for _ in range(1000):\n",
    "  batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "  sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
    "    \n",
    "#\n",
    "# Evaluate\n",
    "#\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
