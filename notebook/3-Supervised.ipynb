{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Supervised Learning\n",
    "\n",
    "In this chapter we will used the Earth Engine techniques from earlier chapters to train a neural network in a simple classification task. This will involve two steps. First, we will automate the download and labeling tasks. Second, we will design, train, and evaluate a neural network using TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload \n",
    "%autoreload 2\n",
    "\n",
    "# Math Stuff\n",
    "import ee, scipy.misc, random, os\n",
    "import numpy as np\n",
    "\n",
    "# GEE stuff\n",
    "from gee_library import *\n",
    "ee.Initialize()\n",
    "\n",
    "# debug stuff\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "# Multi threading\n",
    "import time\n",
    "from threading import Thread\n",
    "\n",
    "# Machine learning\n",
    "import tensorflow as tf\n",
    "from scipy import misc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating testing and training data\n",
    "\n",
    "Neural networks require huge amounts of data to be effective; a supervised classifier will require that all that data be *labeled* as well. Since labeling that much data would be require a huge amount of work for a human to accomplish, we will automate the task.\n",
    "\n",
    "Our neural network will be trained to classify imagery as one of three classes: farmland, mountains, a city. Defining our classes this way allows us to automate the labeling process; we only have to know the gps bounds of a city and every tile we grab from that area can be labeled \"city.\"\n",
    "\n",
    "The next section will walk though a simple automated-imagery-downloader. It will save the images to your hard drive and generate a CSV file that indexes each image with its label. (The pattern of flat images with label files is common for computer vision datasets.) In this chapeter we will leave the images as flat files and feed them directly into the neural network. The next chapter will look into encapsulating them into a more efficient data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Define where we will save the data. \n",
    "#\n",
    "DATA_DIR=\"data/ch3\"\n",
    "TRAIN_IMG_DIR=os.path.join(DATA_DIR, \"train_imgs\")\n",
    "TRAIN_IMG_LABELS=os.path.join(DATA_DIR, \"train.txt\")\n",
    "TEST_IMG_DIR=os.path.join(DATA_DIR, \"test_imgs\")\n",
    "TEST_IMG_LABELS=os.path.join(DATA_DIR, \"test.txt\")\n",
    "\n",
    "\n",
    "#\n",
    "# We will define areas to grab imagery from. When selecting these areas we\n",
    "# had to be careful that they were large enough that taking random selections from\n",
    "# them wouldn't result in repeated images. \n",
    "#\n",
    "\n",
    "# Cities\n",
    "brooklyn= ((-73.965471, 40.614974), (-73.920207, 40.693991))\n",
    "longisland= ((-73.918610, 40.713007), (-73.840551, 40.775980))\n",
    "queens= ((-73.821792, 40.749724), (-73.760813, 40.780303))\n",
    "sf1= ((-122.453085, 37.719024), (-122.394265, 37.789567))\n",
    "sanjose= ((-122.033408, 37.243222), (-121.832452, 37.414198))\n",
    "sandiego= ((-117.144966, 32.743224), (-117.098079, 32.761772))\n",
    "sandiego2= ((-117.098079, 32.690284), (-117.021168, 32.743224))\n",
    "denver= ((-105.127158, 39.569603), (-104.890206, 39.825191))\n",
    "neworleans= ((-90.229627, 29.967342), (-90.034561, 30.016651))\n",
    "baltimore= ((-76.651899, 39.287861), (-76.609940, 39.311176))\n",
    "# Farmland\n",
    "kentucky= ((-84.479444, 38.110622), (-84.335569, 38.258371))\n",
    "kansas= ((-97.533941, 38.105647), (-96.815051, 38.366043))\n",
    "montana= ((-108.994821, 45.875502), (-108.770538, 46.105918))\n",
    "california= ((-121.789047, 38.223409), (-121.575699, 38.473852))\n",
    "virginia= ((-76.838359, 36.483186), (-76.609497, 36.684365))\n",
    "# Mountains\n",
    "cascades = ((-121.575448, 48.224966), (-120.395554, 48.955637))\n",
    "# bc = ((-126.871931, 50.727633), (-122.548457, 51.421085))\n",
    "sierranevadas = ((-120.479266, 38.206113), (-120.198767, 39.346931))\n",
    "yellowstone = ((-110.042831, 43.716602), (-109.379713, 44.437358))\n",
    "rockies = ((-106.790375, 38.610576), (-106.352140, 39.315902))\n",
    "rockies2 = ((-107.872873, 37.627164), (-106.433726, 38.047526))\n",
    "# jasper = ((-118.961940, 51.490493), (-116.910005, 52.873976))\n",
    "yosemite = ((-119.956063, 37.535445), (-119.282902, 38.176927))\n",
    "\n",
    "#\n",
    "# Now we separate the areas into test and training sets. This separation is very important!\n",
    "# Evaluating a statistical model using the same data you trained it on can lead to overfitting\n",
    "# and poor real-world results.\n",
    "#\n",
    "# We're being extra-careful here by including completely different areas in the test and training\n",
    "# sets. Our downloader is fairly unsophisticated and simply grabs random samples from each area;\n",
    "# there is a chance that the some images might overlap. We also wanted to make sure that the\n",
    "# classifier will be able to generalize to new cities not yet seen.\n",
    "#\n",
    "\n",
    "training_cities = [brooklyn, longisland, queens, sf1, sanjose, sandiego, sandiego2]\n",
    "test_cities = [baltimore, neworleans]\n",
    "\n",
    "training_mountains = [cascades, sierranevadas, yellowstone, rockies]\n",
    "test_mountains=[yosemite, rockies2]\n",
    "\n",
    "training_farms=[montana, kansas, kentucky]\n",
    "test_farms=[virginia, california]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def download_data(gps_bound_list, number_of_examples, directory, delay=2):\n",
    "    \"\"\"\n",
    "    Downloads random tiles from a list of regions to `directory`. This function only downloads\n",
    "    the images; it does not generate the CSV file. \n",
    "    \n",
    "    The process of querying the server for a download URL and then downloading and unzipping the\n",
    "    images takes several seconds per image. We can work on multiple images at once by spawning a\n",
    "    separate thread for each image request. This function shows a simple approach to multi-threading.\n",
    "\n",
    "    Google Earth Engine limits the number of requests that an individual can make. If too many images\n",
    "    are resulting in 429 ServerErrors, try increasing the delay.\n",
    "    \n",
    "    gps_bound_list   (List): list of gps coordinates. For example:\n",
    "            [ ((-73.965, 40.614), (-73.920, 40.693)),\n",
    "              ((-105.127, 39.569), (-104.890, 39.825)) ]\n",
    "    number_of_examples(int): The number of images that will be downloaded.\n",
    "    directory      (string): Images will be saved to this directory.\n",
    "    delay             (int): The delay between requests, in seconds.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    \n",
    "    for i in range(number_of_examples):\n",
    "        if i%100 == 0:\n",
    "            print i\n",
    "        t = Thread(target=save_random_tile_at,\n",
    "                   args=(random.choice(gps_bound_list),\n",
    "                         600,\n",
    "                         50,\n",
    "                         ['R', 'G', 'B'],\n",
    "                         os.path.join(directory, str(i)+'.png')))\n",
    "        t.start()\n",
    "        time.sleep(delay)\n",
    "\n",
    "\n",
    "def save_random_tile_at(coords, meters, pixels, bands, file_name):\n",
    "    \"\"\"\n",
    "    Saves a random (pixels x pixels) image from the region described by coords.\n",
    "    The image will be saved to disk at file_name.\n",
    "    \n",
    "    coords: Two gps points describing a rectangle in the form:\n",
    "        ((longitude_min, latitude_min),(longitude_max, latitude_max))\n",
    "    meters: Each image will depict and area measuring meters x meters \n",
    "    pixels: Each image will measure pixels x pixels\n",
    "    bands: List of requested bands. For example: ['R', 'G', 'B']\n",
    "    file_name: File path where the image should be saved at. For example \"./tmp/IMG0293.png\"\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get random location in box\n",
    "        ((longmin, latmin),(longmax, latmax)) = coords\n",
    "\n",
    "        # get random coords\n",
    "        longitude = random.uniform(longmin, longmax)\n",
    "        latitude = random.uniform(latmin, latmax)\n",
    "\n",
    "        # Calculate resolution\n",
    "        resolution = meters/pixels\n",
    "\n",
    "        # Build the GPS box Geometry object\n",
    "        tile_bounds = square_centered_at(\n",
    "            point = (longitude, latitude),\n",
    "            half_distance = meters / 2\n",
    "        )\n",
    "\n",
    "        # load image collection\n",
    "        monterey_collection = ee.ImageCollection('USDA/NAIP/DOQQ')\\\n",
    "            .filterBounds(tile_bounds)\n",
    "\n",
    "        # request imagery\n",
    "        tiles = img_at_region(monterey_collection, resolution, bands, tile_bounds)\n",
    "\n",
    "        # resize img to requested size\n",
    "        np_band_array = [scipy.misc.imresize(tiles[b], (pixels, pixels)) for b in bands]\n",
    "\n",
    "        # and stack the images in a matrix\n",
    "        img = np.dstack(np_band_array)\n",
    "        \n",
    "        # make sure the image isn't a blank image (some times this happens if you get the coordinates\n",
    "        # wrong and point to an area not covered.)\n",
    "        if np.mean(img) == 0:\n",
    "            raise Exception(\"Blank image detected.\")\n",
    "\n",
    "        # Save image to file\n",
    "        scipy.misc.toimage(img, cmin=0.0, cmax=-1).save(file_name)\n",
    "        \n",
    "    #\n",
    "    # Error Handling\n",
    "    #\n",
    "    except ServerError as e:\n",
    "        print e, file_name, coords\n",
    "    except Exception as e:\n",
    "        print e, file_name, coords\n",
    "    return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grabbing training farms...\n",
      "Grabbing training cities...\n",
      "Grabbing training mountains...\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "Status code 429 ./ch3_data/train_imgs/mountains/1269.png ((-110.042831, 43.716602), (-109.379713, 44.437358))\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "Grabbing testing farms...\n",
      "Grabbing testing cities...\n",
      "Grabbing testing mountains...\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Now we'll use all the functions and lists we made to download all of our data.\n",
    "# Keep in mind this is a long process and will take several hours. You can\n",
    "# expediate the process by reducing the number of examples (but you may get worse results.)\n",
    "#\n",
    "\n",
    "#\n",
    "# Download training data\n",
    "#\n",
    "number_per_training_class = 2000\n",
    "print 'Grabbing training farms...'\n",
    "# download_data(training_farms, number_per_training_class, os.path.join(TRAIN_IMG_DIR,'farms'))\n",
    "print 'Grabbing training cities...'\n",
    "# download_data(training_cities, number_per_training_class, os.path.join(TRAIN_IMG_DIR,'cities'))\n",
    "print 'Grabbing training mountains...'\n",
    "download_data(training_mountains, number_per_training_class, os.path.join(TRAIN_IMG_DIR,'mountains'))\n",
    "\n",
    "\n",
    "#\n",
    "# Download test data\n",
    "#\n",
    "number_per_test_class = 500\n",
    "print 'Grabbing testing farms...'\n",
    "# download_data(test_farms, number_per_test_class, os.path.join(TEST_IMG_DIR,'farms'))\n",
    "print 'Grabbing testing cities...'\n",
    "# download_data(test_cities, number_per_test_class, os.path.join(TEST_IMG_DIR,'cities'))\n",
    "print 'Grabbing testing mountains...'\n",
    "download_data(test_mountains, number_per_test_class, os.path.join(TEST_IMG_DIR,'mountains'))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Populate label files as comma-separated values (CSV).\n",
    "# We could have done this step at the same time we downloaded the files, but multiple\n",
    "# threads writing to the same file can be complicated to do right. Instead, we downloaded\n",
    "# each class of image to a separate directory. Since we know the label for each image in\n",
    "# a directory, we can generate the CSV file here.\n",
    "#\n",
    "\n",
    "# Training Data\n",
    "with open(TRAIN_IMG_LABELS, \"w\") as myfile:\n",
    "    klass = 0   # 'class' is a reserved keyword in Python.\n",
    "    path = os.path.join(TRAIN_IMG_DIR,'farms')\n",
    "    for f in os.listdir(path):\n",
    "        filename = os.path.join(path, f)\n",
    "        myfile.write(filename + \",\" + str(klass) + \"\\n\")\n",
    "            \n",
    "    klass = 1\n",
    "    path = os.path.join(TRAIN_IMG_DIR,'cities')\n",
    "    for f in os.listdir(path):\n",
    "        filename = os.path.join(path, f)\n",
    "        myfile.write(filename + \",\" + str(klass) + \"\\n\")\n",
    "        \n",
    "    klass = 2\n",
    "    path = os.path.join(TRAIN_IMG_DIR,'mountains')\n",
    "    for f in os.listdir(path):\n",
    "        filename = os.path.join(path, f)\n",
    "        myfile.write(filename + \",\" + str(klass) + \"\\n\")\n",
    "        \n",
    "\n",
    "# Test Data\n",
    "with open(TEST_IMG_LABELS, \"w\") as myfile:\n",
    "    klass = 0\n",
    "    path = os.path.join(TEST_IMG_DIR,'farms')\n",
    "    for f in os.listdir(path):\n",
    "        filename = os.path.join(path, f)\n",
    "        myfile.write(filename + \",\" + str(klass) + \"\\n\")\n",
    "            \n",
    "    klass = 1\n",
    "    path = os.path.join(TEST_IMG_DIR,'cities')\n",
    "    for f in os.listdir(path):\n",
    "        filename = os.path.join(path, f)\n",
    "        myfile.write(filename + \",\" + str(klass) + \"\\n\")\n",
    "        \n",
    "    klass = 2\n",
    "    path = os.path.join(TEST_IMG_DIR,'mountains')\n",
    "    for f in os.listdir(path):\n",
    "        filename = os.path.join(path, f)\n",
    "        myfile.write(filename + \",\" + str(klass) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "\n",
    "Now that we have downloaded all of our imagery, we can design, train, and evaluate a simple convolutional neural network to classify them.\n",
    "\n",
    "This section will assume that you know the general vocabulary of neural networks and statistics, but not TensorFlow. The math underpining neural networks is an entire field in itself and can't be covered here. If you're interested in learning the science, Stanford makes its excellent CS231n available online for free: https://cs231n.github.io/. If that course is a bit too advance, Andrew Ng's Machine Learning course on Coursera is a very good introduction to Machine Learning in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Model\n",
    "#\n",
    "\n",
    "\n",
    "#\n",
    "# By convention, x is used to refer to the input to a statistical model\n",
    "# and y refers to the model's output. Since this is supervised learning\n",
    "# we must provide labels for the data. These labels can be thought of\n",
    "# as the \"correct answers.\" We will use y_ to describe these.\n",
    "#\n",
    "# Placeholders are variables in a TensorFlow model that a user must fill\n",
    "# before training or testing the model. (We will provided values for these\n",
    "# place hodlers later.)\n",
    "#\n",
    "# Keep in mind that TensorFlow works using a concept it calls 'defered execution.'\n",
    "# This means that as we call functions like tf.reshape() and tf.multiply(), no\n",
    "# computations are actually happening yet. Tensorflow records the operations and\n",
    "# connections we make in this section and executes them noly when we train and then test\n",
    "# the network later.\n",
    "#\n",
    "\n",
    "# Our input images will be a 50x50x3 image flattened into a vector\n",
    "x = tf.placeholder(tf.float32, [None, 50*50*3])\n",
    "# The answers will be given as a one-hot-array. This is an array where exactly one\n",
    "# element is 1 and the rest are zeros. For example: [0,1,0]\n",
    "y_ = tf.placeholder(tf.float32, [None, 3])\n",
    "# The training placeholder will allow us to turn on and off certain network features\n",
    "# like dropout and batch normalization\n",
    "training = tf.placeholder(tf.bool)\n",
    "\n",
    "\n",
    "input_norm = tf.contrib.layers.batch_norm(x, \n",
    "                                  center=True, scale=True, \n",
    "                                  is_training=training)\n",
    "\n",
    "\n",
    "# Since we are using a convolutional neural network we must convert the flattened image\n",
    "# into a proper image shape.\n",
    "input_layer = tf.reshape(input_norm, [-1, 50, 50, 3])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "# Convolution and Pooling Layers\n",
    "#\n",
    "\n",
    "# Convolutional Layer #1\n",
    "conv1 = tf.layers.conv2d(\n",
    "    inputs=input_layer,\n",
    "    filters=32,\n",
    "    kernel_size=[3, 3],\n",
    "    padding=\"valid\",\n",
    "    activation=tf.nn.relu)\n",
    "\n",
    "# Pooling Layer #1\n",
    "pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
    "# pool1_norm = tf.contrib.layers.batch_norm(pool1, center=True, scale=True, activation=tf.nn.relu, is_training=training)\n",
    "\n",
    "# Convolutional Layer #2 and Pooling Layer #2\n",
    "conv2 = tf.layers.conv2d(\n",
    "    inputs=pool1,\n",
    "    filters=64,\n",
    "    kernel_size=[3, 3],\n",
    "    padding=\"valid\",\n",
    "    activation=tf.nn.relu)\n",
    "pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
    "# pool2_norm = tf.contrib.layers.batch_norm(pool2, center=True, scale=True, activation=tf.nn.relu, is_training=training)\n",
    "\n",
    "# Convolutional Layer #3 (no pooling)\n",
    "conv3 = tf.layers.conv2d(\n",
    "    inputs=pool2,\n",
    "    filters=64,\n",
    "    kernel_size=[3, 3],\n",
    "    padding=\"valid\",\n",
    "    activation=tf.nn.relu)\n",
    "# conv3_norm = tf.contrib.layers.batch_norm(conv3, center=True, scale=True, activation=tf.nn.relu, is_training=training)\n",
    "\n",
    "\n",
    "#\n",
    "# Fully Connected Layers\n",
    "#\n",
    "\n",
    "\n",
    "# Dense Layer\n",
    "conv3_flat = tf.reshape(conv3, [-1, 9 * 9 * 64])\n",
    "fc1 = tf.layers.dense(inputs=conv3_flat, units=1024, activation=tf.nn.relu)\n",
    "# fc1_norm = tf.contrib.layers.batch_norm(fc1, center=True, scale=True, activation=tf.nn.relu, is_training=training)\n",
    "dropout = tf.layers.dropout(\n",
    "    inputs=fc1,\n",
    "    rate=0.6,\n",
    "    training= True)\n",
    "\n",
    "# Model Output. We want this to be a one-hot array.\n",
    "y = tf.layers.dense(inputs=dropout, units=3)\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "# The loss function, sometimes called a cost function, measures how good the network's\n",
    "# answer compares to the correct answer. A lower loss means a better answer.\n",
    "# A good loss function smoothly approaches zero as two answers approach each other.\n",
    "# 'Softmax' refers to the process of turing the y array, which could contain any values,\n",
    "# into a matrix where all the elements sum to 1. Since every image belongs only to one class,\n",
    "# this makes sense.\n",
    "# We will be using a common classifier loss function: the cross-entropy loss. It's not\n",
    "# too important to know how it works right now.\n",
    "#\n",
    "\n",
    "loss = None\n",
    "train_op = None\n",
    "\n",
    "loss = tf.losses.softmax_cross_entropy(\n",
    "    onehot_labels=y_, # ground truth\n",
    "    logits=y) # network output\n",
    "\n",
    "#\n",
    "# Here we define an accuracy function so that we can have some\n",
    "# visibility into how well our model is learning. This is used during\n",
    "# the training step.\n",
    "#\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "model_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# We'll also calculate the softmax of the output for easier predictions\n",
    "y_softmax = tf.nn.softmax(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# It is common to train a neural network on \"batches\" of images. This creates\n",
    "# smoother gradients and expedites learning. To batch images we will stack 3-dimentional\n",
    "# images (width x height x channels) in the fourth dimention to create a 4-dimentional tensor\n",
    "# with the dimentions (batch_size x height x width x channels)\n",
    "#\n",
    "# We'll define a function to batch images for us, which we will use in the next section.\n",
    "#\n",
    "\n",
    "def gee_batch(data_label_file, batch_size):\n",
    "    \"\"\"\n",
    "    Generates image batches.\n",
    "    \n",
    "    data_label_file: File path of CSV file detailing filenames and labels\n",
    "    batch_size: Number of images to stack\n",
    "    \"\"\"\n",
    "    \n",
    "    # read CSV file\n",
    "    lines = open(data_label_file).read().splitlines()\n",
    "    \n",
    "    img_list = []\n",
    "    label_list = []\n",
    "    for i in range(batch_size):\n",
    "        # Choose a random image from the dataset\n",
    "        png_path, label = random.choice(lines).split(',')\n",
    "        # Load the image\n",
    "        img_list.append(misc.imread(png_path).flatten())\n",
    "        # And generate a one-hot array for the label\n",
    "        one_hot = np.zeros(3)\n",
    "        one_hot[int(label)] = 1\n",
    "        label_list.append(one_hot)\n",
    "        \n",
    "    return np.stack(img_list, axis=0), np.stack(label_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 Loss: 30.7566 Accuracy: 0.328125 LR: 0.0001\n",
      "Step 100 Loss: 11.312 Accuracy: 0.46875 LR: 9.6e-05\n",
      "Step 200 Loss: 7.03179 Accuracy: 0.4375 LR: 9.216e-05\n",
      "Step 300 Loss: 7.87562 Accuracy: 0.476562 LR: 8.84736e-05\n",
      "Step 400 Loss: 5.31416 Accuracy: 0.46875 LR: 8.49346e-05\n",
      "Step 500 Loss: 4.84675 Accuracy: 0.414062 LR: 8.15373e-05\n",
      "Step 600 Loss: 4.0106 Accuracy: 0.554688 LR: 7.82758e-05\n",
      "Step 700 Loss: 2.61514 Accuracy: 0.601562 LR: 7.51447e-05\n",
      "Step 800 Loss: 2.68239 Accuracy: 0.59375 LR: 7.21389e-05\n",
      "Step 900 Loss: 2.38447 Accuracy: 0.632812 LR: 6.92534e-05\n",
      "Step 1000 Loss: 1.36465 Accuracy: 0.726562 LR: 6.64832e-05\n",
      "Step 1100 Loss: 1.63842 Accuracy: 0.65625 LR: 6.38239e-05\n",
      "Step 1200 Loss: 1.4228 Accuracy: 0.679688 LR: 6.1271e-05\n",
      "Step 1300 Loss: 0.88696 Accuracy: 0.757812 LR: 5.88201e-05\n",
      "Step 1400 Loss: 0.829078 Accuracy: 0.695312 LR: 5.64673e-05\n",
      "Step 1500 Loss: 0.649436 Accuracy: 0.804688 LR: 5.42086e-05\n",
      "Step 1600 Loss: 0.548211 Accuracy: 0.804688 LR: 5.20403e-05\n",
      "Step 1700 Loss: 0.480668 Accuracy: 0.828125 LR: 4.99587e-05\n",
      "Step 1800 Loss: 0.476524 Accuracy: 0.84375 LR: 4.79603e-05\n",
      "Step 1900 Loss: 0.272639 Accuracy: 0.867188 LR: 4.60419e-05\n",
      "Step 2000 Loss: 0.32023 Accuracy: 0.882812 LR: 4.42002e-05\n",
      "Step 2100 Loss: 0.144175 Accuracy: 0.960938 LR: 4.24322e-05\n",
      "Step 2200 Loss: 0.122891 Accuracy: 0.9375 LR: 4.07349e-05\n",
      "Step 2300 Loss: 0.0600581 Accuracy: 0.984375 LR: 3.91055e-05\n",
      "Step 2400 Loss: 0.066137 Accuracy: 0.976562 LR: 3.75413e-05\n",
      "Step 2500 Loss: 0.0460745 Accuracy: 0.992188 LR: 3.60396e-05\n",
      "Step 2600 Loss: 0.0235686 Accuracy: 1.0 LR: 3.45981e-05\n",
      "Step 2700 Loss: 0.0271577 Accuracy: 0.992188 LR: 3.32141e-05\n",
      "Step 2800 Loss: 0.0324698 Accuracy: 0.992188 LR: 3.18856e-05\n",
      "Step 2900 Loss: 0.0207118 Accuracy: 0.992188 LR: 3.06102e-05\n",
      "Step 3000 Loss: 0.0234678 Accuracy: 1.0 LR: 2.93857e-05\n",
      "Step 3100 Loss: 0.0358536 Accuracy: 0.992188 LR: 2.82103e-05\n",
      "Step 3200 Loss: 0.01988 Accuracy: 1.0 LR: 2.70819e-05\n",
      "Step 3300 Loss: 0.0142802 Accuracy: 1.0 LR: 2.59986e-05\n",
      "Step 3400 Loss: 0.0068362 Accuracy: 1.0 LR: 2.49587e-05\n",
      "Step 3500 Loss: 0.0331242 Accuracy: 0.992188 LR: 2.39603e-05\n",
      "Step 3600 Loss: 0.0171574 Accuracy: 1.0 LR: 2.30019e-05\n",
      "Step 3700 Loss: 0.0109147 Accuracy: 1.0 LR: 2.20818e-05\n",
      "Step 3800 Loss: 0.00763176 Accuracy: 1.0 LR: 2.11986e-05\n",
      "Step 3900 Loss: 0.0193005 Accuracy: 0.992188 LR: 2.03506e-05\n",
      "Step 4000 Loss: 0.0206747 Accuracy: 0.992188 LR: 1.95366e-05\n",
      "Step 4100 Loss: 0.012867 Accuracy: 1.0 LR: 1.87551e-05\n",
      "Step 4200 Loss: 0.00658285 Accuracy: 1.0 LR: 1.80049e-05\n",
      "Step 4300 Loss: 0.00816004 Accuracy: 1.0 LR: 1.72847e-05\n",
      "Step 4400 Loss: 0.0106446 Accuracy: 1.0 LR: 1.65933e-05\n",
      "Step 4500 Loss: 0.0111599 Accuracy: 0.992188 LR: 1.59296e-05\n",
      "Step 4600 Loss: 0.00881413 Accuracy: 1.0 LR: 1.52924e-05\n",
      "Step 4700 Loss: 0.00536745 Accuracy: 1.0 LR: 1.46807e-05\n",
      "Step 4800 Loss: 0.0142778 Accuracy: 0.992188 LR: 1.40935e-05\n",
      "Step 4900 Loss: 0.00351157 Accuracy: 1.0 LR: 1.35298e-05\n",
      "Step 5000 Loss: 0.0200003 Accuracy: 0.992188 LR: 1.29886e-05\n",
      "Step 5100 Loss: 0.00988716 Accuracy: 1.0 LR: 1.2469e-05\n",
      "Step 5200 Loss: 0.00329635 Accuracy: 1.0 LR: 1.19703e-05\n",
      "Step 5300 Loss: 0.00738666 Accuracy: 1.0 LR: 1.14914e-05\n",
      "Step 5400 Loss: 0.00751458 Accuracy: 1.0 LR: 1.10318e-05\n",
      "Step 5500 Loss: 0.0050955 Accuracy: 1.0 LR: 1.05905e-05\n",
      "Step 5600 Loss: 0.00663803 Accuracy: 1.0 LR: 1.01669e-05\n",
      "Step 5700 Loss: 0.00407992 Accuracy: 1.0 LR: 9.76022e-06\n",
      "Step 5800 Loss: 0.00993434 Accuracy: 1.0 LR: 9.36981e-06\n",
      "Step 5900 Loss: 0.00554931 Accuracy: 1.0 LR: 8.99502e-06\n",
      "Step 6000 Loss: 0.0036045 Accuracy: 1.0 LR: 8.63522e-06\n",
      "Step 6100 Loss: 0.00577736 Accuracy: 1.0 LR: 8.28981e-06\n",
      "Step 6200 Loss: 0.00888084 Accuracy: 1.0 LR: 7.95822e-06\n",
      "Step 6300 Loss: 0.00338532 Accuracy: 1.0 LR: 7.63989e-06\n",
      "Step 6400 Loss: 0.00506233 Accuracy: 1.0 LR: 7.33429e-06\n",
      "Step 6500 Loss: 0.0170643 Accuracy: 0.992188 LR: 7.04092e-06\n",
      "Step 6600 Loss: 0.00746326 Accuracy: 1.0 LR: 6.75928e-06\n",
      "Step 6700 Loss: 0.00353835 Accuracy: 1.0 LR: 6.48891e-06\n",
      "Step 6800 Loss: 0.00466943 Accuracy: 1.0 LR: 6.22936e-06\n",
      "Step 6900 Loss: 0.00384648 Accuracy: 1.0 LR: 5.98018e-06\n",
      "Step 7000 Loss: 0.00317319 Accuracy: 1.0 LR: 5.74097e-06\n",
      "Step 7100 Loss: 0.00210903 Accuracy: 1.0 LR: 5.51134e-06\n",
      "Step 7200 Loss: 0.00944587 Accuracy: 1.0 LR: 5.29088e-06\n",
      "Step 7300 Loss: 0.00536627 Accuracy: 1.0 LR: 5.07925e-06\n",
      "Step 7400 Loss: 0.00138379 Accuracy: 1.0 LR: 4.87608e-06\n",
      "Step 7500 Loss: 0.00342182 Accuracy: 1.0 LR: 4.68103e-06\n",
      "Step 7600 Loss: 0.00562952 Accuracy: 1.0 LR: 4.49379e-06\n",
      "Step 7700 Loss: 0.00399996 Accuracy: 1.0 LR: 4.31404e-06\n",
      "Step 7800 Loss: 0.00191877 Accuracy: 1.0 LR: 4.14148e-06\n",
      "Step 7900 Loss: 0.00394821 Accuracy: 1.0 LR: 3.97582e-06\n",
      "Step 8000 Loss: 0.00820625 Accuracy: 0.992188 LR: 3.81679e-06\n",
      "Step 8100 Loss: 0.00385572 Accuracy: 1.0 LR: 3.66411e-06\n",
      "Step 8200 Loss: 0.00416844 Accuracy: 1.0 LR: 3.51755e-06\n",
      "Step 8300 Loss: 0.00670302 Accuracy: 1.0 LR: 3.37685e-06\n",
      "Step 8400 Loss: 0.00358668 Accuracy: 1.0 LR: 3.24177e-06\n",
      "Step 8500 Loss: 0.00207688 Accuracy: 1.0 LR: 3.1121e-06\n",
      "Step 8600 Loss: 0.00381739 Accuracy: 1.0 LR: 2.98762e-06\n",
      "Step 8700 Loss: 0.00337994 Accuracy: 1.0 LR: 2.86811e-06\n",
      "Step 8800 Loss: 0.0032959 Accuracy: 1.0 LR: 2.75339e-06\n",
      "Step 8900 Loss: 0.00305397 Accuracy: 1.0 LR: 2.64325e-06\n",
      "Step 9000 Loss: 0.0104321 Accuracy: 1.0 LR: 2.53752e-06\n",
      "Step 9100 Loss: 0.00469054 Accuracy: 1.0 LR: 2.43602e-06\n",
      "Step 9200 Loss: 0.0132564 Accuracy: 0.992188 LR: 2.33858e-06\n",
      "Step 9300 Loss: 0.00333527 Accuracy: 1.0 LR: 2.24504e-06\n",
      "Step 9400 Loss: 0.00207407 Accuracy: 1.0 LR: 2.15524e-06\n",
      "Step 9500 Loss: 0.00329445 Accuracy: 1.0 LR: 2.06903e-06\n",
      "Step 9600 Loss: 0.00419215 Accuracy: 1.0 LR: 1.98627e-06\n",
      "Step 9700 Loss: 0.00156389 Accuracy: 1.0 LR: 1.90682e-06\n",
      "Step 9800 Loss: 0.00248844 Accuracy: 1.0 LR: 1.83054e-06\n",
      "Step 9900 Loss: 0.00184349 Accuracy: 1.0 LR: 1.75732e-06\n",
      "Step 10000 Loss: 0.00284559 Accuracy: 1.0 LR: 1.68703e-06\n",
      "Step 10100 Loss: 0.00255615 Accuracy: 1.0 LR: 1.61955e-06\n",
      "Step 10200 Loss: 0.00464011 Accuracy: 1.0 LR: 1.55477e-06\n",
      "Step 10300 Loss: 0.00149689 Accuracy: 1.0 LR: 1.49257e-06\n",
      "Step 10400 Loss: 0.002638 Accuracy: 1.0 LR: 1.43287e-06\n",
      "Step 10500 Loss: 0.00201835 Accuracy: 1.0 LR: 1.37556e-06\n",
      "Step 10600 Loss: 0.00463858 Accuracy: 1.0 LR: 1.32053e-06\n",
      "Step 10700 Loss: 0.00723421 Accuracy: 1.0 LR: 1.26771e-06\n",
      "Step 10800 Loss: 0.00275405 Accuracy: 1.0 LR: 1.217e-06\n",
      "Step 10900 Loss: 0.00157359 Accuracy: 1.0 LR: 1.16832e-06\n",
      "Step 11000 Loss: 0.00188889 Accuracy: 1.0 LR: 1.12159e-06\n",
      "Step 11100 Loss: 0.00524457 Accuracy: 1.0 LR: 1.07673e-06\n",
      "Step 11200 Loss: 0.00266712 Accuracy: 1.0 LR: 1.03366e-06\n",
      "Step 11300 Loss: 0.00314316 Accuracy: 1.0 LR: 9.92312e-07\n",
      "Step 11400 Loss: 0.00164098 Accuracy: 1.0 LR: 9.52619e-07\n",
      "Step 11500 Loss: 0.00352546 Accuracy: 1.0 LR: 9.14515e-07\n",
      "Step 11600 Loss: 0.0061479 Accuracy: 1.0 LR: 8.77934e-07\n",
      "Step 11700 Loss: 0.00434468 Accuracy: 1.0 LR: 8.42817e-07\n",
      "Step 11800 Loss: 0.00200662 Accuracy: 1.0 LR: 8.09104e-07\n",
      "Step 11900 Loss: 0.00307593 Accuracy: 1.0 LR: 7.7674e-07\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Training\n",
    "#\n",
    "# Now we'll train the network. All the operations we defined earlier will be\n",
    "# executed in this section. If you're using a CPU for calculations (which you\n",
    "# probably are) this section could take several hours, even for the few thousand\n",
    "# iterations. If you see loss flatten out, though, you can halt the kernel and\n",
    "# continue to to the next cell for evaluation.\n",
    "# (A note on GPUs: Tensorflow only support NVIDIA GPUs right now, so even if you\n",
    "# have a GPU you might not be able to use it. If you do have an NVIDIA GPU you'll\n",
    "# have to recompile Tensorflow with the CUDA drivers. That process won't be\n",
    "# covered here.)\n",
    "# \n",
    "\n",
    "\n",
    "# The 'optimizer' is the algorithm we will use to minimize the cost function.\n",
    "# This is a surprisingly complex issue with many approaches, although Adam\n",
    "# is a good default.\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS) # Required for batch norm\n",
    "with tf.control_dependencies(update_ops): # Required for batch norm\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    starter_learning_rate = 1e-4\n",
    "    learning_rate = tf.train.exponential_decay(learning_rate = starter_learning_rate,\n",
    "                                               global_step = global_step,\n",
    "                                               decay_steps = 100,\n",
    "                                               decay_rate = 0.96,\n",
    "                                               staircase=True)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "\n",
    "# Initialize TensorFlow\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "# And run 20k iterations\n",
    "for i in range(12000):\n",
    "    # Load a batch of images\n",
    "    example_batch, label_batch = gee_batch(TRAIN_IMG_LABELS, 128)\n",
    "    \n",
    "    # Every 100 iterations we evaluate how well the network is performing. We're using\n",
    "    # the test set to display loss and accuracy; expect performance on the test\n",
    "    # set to be worse.\n",
    "    if i%100 == 0:\n",
    "        train_accuracy, loss_out, lr = sess.run( fetches = [model_accuracy, loss, learning_rate],\n",
    "                                       feed_dict={x : example_batch,\n",
    "                                                  y_: label_batch,\n",
    "                                                  training: False})\n",
    "        print \"Step\", i, \"Loss:\", loss_out, \"Accuracy:\", train_accuracy, \"LR:\", lr\n",
    "        \n",
    "    # run an iteration\n",
    "    optimizer.run(feed_dict={x:example_batch,\n",
    "                             y_: label_batch,\n",
    "                             training: True})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in the test set: [0.52734375]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Evaluation\n",
    "# \n",
    "# Now we will evaluate the performance of the network by measuring the accuracy on a batch of\n",
    "# images from the test set.\n",
    "#\n",
    "# I get about 52% accuracy, which is pretty poor (but better than random, which would be 33%.)\n",
    "# See if you can do better!\n",
    "#\n",
    "\n",
    "example_batch, label_batch = gee_batch(TEST_IMG_LABELS, 256)\n",
    "\n",
    "train_accuracy = sess.run( fetches = [model_accuracy],\n",
    "                                     feed_dict={x : example_batch,\n",
    "                                                y_: label_batch,\n",
    "                                                training: False})\n",
    "\n",
    "\n",
    "print \"Accuracy in the test set:\", train_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f20d93115d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions: [0.0, 0.4321, 0.5679]\n",
      "ground truth: [ 0.  0.  1.]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Prediction\n",
    "#\n",
    "# Now we will use the model to predict the class of some images from the test set.\n",
    "# We'll keep the one-hot array encoding for the labels so that you can see the network's\n",
    "# output. Higher predicted values mean that the network is more confident that the image\n",
    "# belongs to that class.\n",
    "# As a reminder, the classes are:\n",
    "# farm:     0  one-hot: [1,0,0]\n",
    "# city:     1  one-hot: [0,1,0]\n",
    "# mountain: 2  one-hot: [0,0,1]\n",
    "#\n",
    "# You can keep rerunning this cell to test more images (CTRL-Enter runs the selected cell.)\n",
    "\n",
    "\n",
    "example_batch, label_batch = gee_batch(TEST_IMG_LABELS, 1)\n",
    "\n",
    "groundtruth, predicted = sess.run(   fetches = [y_, y_softmax],\n",
    "                                     feed_dict={x : example_batch,\n",
    "                                                y_: label_batch,\n",
    "                                                training: False})\n",
    "\n",
    "\n",
    "img = example_batch.reshape((50,50,3))\n",
    "plt.imshow(img); plt.show()\n",
    "\n",
    "print \"predictions:\", [ round(elem, 4) for elem in predicted[0] ]\n",
    "print \"ground truth:\", groundtruth[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
