{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment\n",
    "\n",
    "## Pipelined processor\n",
    "\n",
    "This file defines a neural network that processes several specrums individually. For each example, individual channels are sent through their own neural network. To use this system for classification, the output of all the neural networks must be combined with a post-processor. \n",
    "\n",
    "In this experiment we are processing 5 different channels: Visual red, visual green, visual blue, altitude data, and night-time near-IR data (city lights). The data was collected and saved as a TFRecord previously in the Data Aquisition chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpp\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload \n",
    "%autoreload 2\n",
    "\n",
    "# Math Stuff\n",
    "import scipy.misc, random, os\n",
    "import numpy as np\n",
    "\n",
    "# Google Earth Engine\n",
    "import ee\n",
    "from gee_library import *\n",
    "ee.Initialize()\n",
    "\n",
    "# debug stuff\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "# Threads\n",
    "import time, Queue\n",
    "from tqdm import trange\n",
    "from threading import Thread\n",
    "\n",
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "from google.protobuf.internal import api_implementation\n",
    "print(api_implementation._default_implementation_type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_DIR=\"data/experiment\"\n",
    "TEST_PROTO_FILENAME = os.path.join(DATA_DIR,\"multi_spectrum_test.tfrecords\")\n",
    "TRAIN_PROTO_FILENAME = os.path.join(DATA_DIR,\"multi_spectrum_train.tfrecords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "\n",
    "The `get_batch()` function reads from the TFRecord file we created before and extracts a batch of examples. It delivers each channel separatly.\n",
    "\n",
    "This function is hard-coded for the spectrums used in this experiment to maximize readibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(proto_filename, batch_size):\n",
    "\n",
    "    NUMBER_OF_CLASSES=3\n",
    "    \n",
    "    filename_queue = tf.train.string_input_producer([proto_filename], num_epochs=None)\n",
    "    proto_reader = tf.TFRecordReader()\n",
    "\n",
    "    # get an example from file\n",
    "    _, serialized_example = proto_reader.read(filename_queue)\n",
    "\n",
    "    # unpack it\n",
    "    features = tf.parse_single_example(\n",
    "        serialized_example,\n",
    "        features={\n",
    "            # We know the length of both fields. If not the\n",
    "            # tf.VarLenFeature could be used\n",
    "            'label': tf.FixedLenFeature([], tf.int64),\n",
    "            'R': tf.FixedLenFeature([50*50], tf.int64),\n",
    "            'G': tf.FixedLenFeature([50*50], tf.int64),\n",
    "            'B': tf.FixedLenFeature([50*50], tf.int64),\n",
    "            'elevation': tf.FixedLenFeature([50*50], tf.int64),\n",
    "            'nightlights': tf.FixedLenFeature([50*50], tf.int64),\n",
    "        })\n",
    "\n",
    "    # now we have the raw data. Wrangle it into the right dimentions\n",
    "    R = tf.divide(tf.cast(features['R'], tf.float32), 255)\n",
    "    G = tf.divide(tf.cast(features['G'], tf.float32), 255)\n",
    "    B = tf.divide(tf.cast(features['B'], tf.float32), 255)\n",
    "    elevation = tf.divide(tf.cast(features['elevation'], tf.float32), 255)\n",
    "    nightlights = tf.divide(tf.cast(features['nightlights'], tf.float32), 255)\n",
    "    \n",
    "    # The label is an integer; convert it to a one-hot array\n",
    "    label = tf.one_hot(features['label'], NUMBER_OF_CLASSES)\n",
    "\n",
    "    # and batch it\n",
    "    R_batch, G_batch, B_batch, elevation_batch, nightlight_batch, labels_batch = tf.train.shuffle_batch(\n",
    "        [R, G, B, elevation, nightlights, label],\n",
    "        batch_size=batch_size,\n",
    "        capacity=2000,\n",
    "        min_after_dequeue=1000)\n",
    "\n",
    "    return R_batch, G_batch, B_batch, elevation_batch, nightlight_batch, labels_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Definition\n",
    "\n",
    "Now we will define the pipelined neural network. For simplicity, this example will use identical networks for each channel. Instead of writing out the network definition 5 times, though, we will define a function that takes data as input and returns the network output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Required input placeholders\n",
    "#\n",
    "training = tf.placeholder(dtype=tf.bool, name=\"is_training\") # True if training, False if testing\n",
    "batch_size = tf.placeholder(dtype=tf.int32, name=\"batch_size\")\n",
    "\n",
    "\n",
    "##############\n",
    "# Data Input #\n",
    "##############\n",
    "\n",
    "# Define variables for both the traning and test batches. We will only use one of them per iteration, though.\n",
    "R_train_batch, G_train_batch, B_train_batch, elevation_train_batch, nightlight_train_batch, labels_train_batch = get_batch(TRAIN_PROTO_FILENAME, batch_size)\n",
    "R_test_batch, G_test_batch, B_test_batch, elevation_test_batch, nightlight_test_batch, labels_test_batch = get_batch(TRAIN_PROTO_FILENAME, batch_size)\n",
    "\n",
    "# Use a conditional tensor to select between using the test and training data\n",
    "R_batch, G_batch, B_batch, elevation_batch, nightlight_batch, labels_batch = tf.cond(training,\n",
    "                         lambda: (R_train_batch, G_train_batch, B_train_batch, elevation_train_batch, nightlight_train_batch, labels_train_batch), # training==True\n",
    "                         lambda: (R_test_batch,  G_test_batch,  B_test_batch,  elevation_test_batch,  nightlight_test_batch,  labels_test_batch)# training==False\n",
    "                        )\n",
    "\n",
    "\n",
    "\n",
    "####################\n",
    "# Model Definition #\n",
    "####################\n",
    "\n",
    "def pipelined_model(x, y_):\n",
    "\n",
    "    # Batch Normalization\n",
    "    input_norm = tf.contrib.layers.batch_norm(x, \n",
    "                                      center=True, scale=True, \n",
    "                                      is_training=training)\n",
    "\n",
    "\n",
    "\n",
    "    # Resize the data from 2500 element vectors to 2D images.\n",
    "    input_layer = tf.reshape(input_norm, [-1, 50, 50, 1])\n",
    "\n",
    "\n",
    "    #\n",
    "    # Convolution and Pooling Layers\n",
    "    #\n",
    "\n",
    "    # Convolutional Layer #1\n",
    "    conv1 = tf.layers.conv2d(\n",
    "        inputs=input_layer,\n",
    "        filters=32,\n",
    "        kernel_size=[3, 3],\n",
    "        padding=\"valid\",\n",
    "        activation=tf.nn.relu)\n",
    "\n",
    "    # Pooling Layer #1\n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
    "    # pool1_norm = tf.contrib.layers.batch_norm(pool1, center=True, scale=True, activation=tf.nn.relu, is_training=training)\n",
    "\n",
    "    # Convolutional Layer #2 and Pooling Layer #2\n",
    "    conv2 = tf.layers.conv2d(\n",
    "        inputs=pool1,\n",
    "        filters=64,\n",
    "        kernel_size=[3, 3],\n",
    "        padding=\"valid\",\n",
    "        activation=tf.nn.relu)\n",
    "    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
    "    # pool2_norm = tf.contrib.layers.batch_norm(pool2, center=True, scale=True, activation=tf.nn.relu, is_training=training)\n",
    "\n",
    "    # Convolutional Layer #3 (no pooling)\n",
    "    conv3 = tf.layers.conv2d(\n",
    "        inputs=pool2,\n",
    "        filters=64,\n",
    "        kernel_size=[3, 3],\n",
    "        padding=\"valid\",\n",
    "        activation=tf.nn.relu)\n",
    "    # conv3_norm = tf.contrib.layers.batch_norm(conv3, center=True, scale=True, activation=tf.nn.relu, is_training=training)\n",
    "\n",
    "\n",
    "    #\n",
    "    # Fully Connected Layers\n",
    "    #\n",
    "\n",
    "\n",
    "    # Dense Layer\n",
    "    conv3_flat = tf.reshape(conv3, [-1, 9 * 9 * 64])\n",
    "    fc1 = tf.layers.dense(inputs=conv3_flat, units=1024, activation=tf.nn.relu)\n",
    "    # fc1_norm = tf.contrib.layers.batch_norm(fc1, center=True, scale=True, activation=tf.nn.relu, is_training=training)\n",
    "    dropout = tf.layers.dropout(\n",
    "        inputs=fc1,\n",
    "        rate=0.6,\n",
    "        training= True)\n",
    "\n",
    "    # Model Output. This will be a one-hot array.\n",
    "    y = tf.layers.dense(inputs=dropout, units=3)\n",
    "\n",
    "\n",
    "\n",
    "    #\n",
    "    # Loss\n",
    "    #\n",
    "\n",
    "    loss = None\n",
    "    train_op = None\n",
    "\n",
    "    loss = tf.losses.softmax_cross_entropy(\n",
    "        onehot_labels=y_, # ground truth\n",
    "        logits=y) # network output\n",
    "\n",
    "\n",
    "    #\n",
    "    # Accuracy Output; helpful for observing performance\n",
    "    # \n",
    "\n",
    "    correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "    model_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    return y, loss, model_accuracy\n",
    "    \n",
    "    \n",
    "    \n",
    "#\n",
    "# Now that we have defined the model as a function, we will feed\n",
    "# each layer into it individually.\n",
    "#\n",
    "\n",
    "    # Here's where we stack the channels to create a 50x50x5 pixel example batch.\n",
    "    x = tf.stack(values=[R_batch, G_batch, B_batch, elevation_batch, nightlight_batch],\n",
    "                 axis=2)\n",
    "    y_ = labels_batch\n",
    "\n",
    "\n",
    "R_y, R_loss, R_accuracy = pipelined_model(R_batch, labels_batch)\n",
    "G_y, G_loss, G_accuracy = pipelined_model(G_batch, labels_batch)\n",
    "B_y, B_loss, B_accuracy = pipelined_model(B_batch, labels_batch)\n",
    "E_y, E_loss, E_accuracy = pipelined_model(elevation_batch, labels_batch)\n",
    "N_y, N_loss, N_accuracy = pipelined_model(nightlight_batch, labels_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 Loss: [5.4942713]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Train\n",
    "#\n",
    "\n",
    "# Since we have 5 different pipelines, each processing different channels, we\n",
    "# could process each individually, one after another. However, we can define\n",
    "# an \"overall\" loss to minimize so that Tensorflow trains all 5 networks simultaneously.\n",
    "overall_loss = tf.reduce_mean(R_loss + G_loss + B_loss + E_loss + N_loss)\n",
    "\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS) # Required for batch norm\n",
    "with tf.control_dependencies(update_ops): # Required for batch norm\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    starter_learning_rate = 1e-5\n",
    "    learning_rate = tf.train.exponential_decay(learning_rate = starter_learning_rate,\n",
    "                                               global_step = global_step,\n",
    "                                               decay_steps = 200,\n",
    "                                               decay_rate = 0.96,\n",
    "                                               staircase=True)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(overall_loss, global_step=global_step)\n",
    "\n",
    "\n",
    "# Initialize TensorFlow\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# start the data reader tensors\n",
    "tf.train.start_queue_runners(sess=sess)\n",
    "\n",
    "# And run 20k iterations\n",
    "for i in range(12000):\n",
    "    if i%100 == 0:\n",
    "        loss_out = sess.run( fetches = [overall_loss],\n",
    "                             feed_dict={training: False, batch_size:128})\n",
    "        print \"Step\", i, \"Loss:\", loss_out\n",
    "        \n",
    "    # run an iteration\n",
    "    optimizer.run(feed_dict={batch_size:128,\n",
    "                             training: True})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Evaluate\n",
    "#\n",
    "\n",
    "\n",
    "train_accuracy = sess.run( fetches = [model_accuracy],\n",
    "                           feed_dict={training: False, batch_size:128})\n",
    "\n",
    "\n",
    "print \"Accuracy in the test set:\", train_accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
