{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning\n",
    "\n",
    "In the previous chapter we used a neural network to classify images into one of three categories. We used a semi-automated method to label the data going into the network since data-labeling and cleansing is one of the most time-intensive and error-prone steps in machine learning.\n",
    "\n",
    "But what if we didn't have access to labeled data, or what if we didn't know what labels should be applied to a dataset? This is the realm of \"unsupervised learning,\" where an algorithm is given just the data and no labels. Clustering is one of the more common unsupervised tasks, where an algorithm is asked to collect data into natural groupings.\n",
    "\n",
    "We will be using a special neural network called an \"autoencoder\" in the exercise. Instead of outputting a classification, an autoencoder attempts to reconstruct the original data as best it can. What makes this an interesting task is that the network contains a \"bottleneck,\" or a section with fewer neurons than input/output pixels. This means that the network has to encode large areas of an image into small encodings. You probably can guess that autoencoders can be very useful for image compression and noise reduction.\n",
    "\n",
    "What we will do in this experiment is build and train an autoencoder using the same data as Chapter 3. After the autoencoder is trained we will inspect its \"bottleneck,\" which will be a fully connected layer. We will treat the activations in this layer as a feature vector that we will feed into a traditional clustering algorithm.\n",
    "\n",
    "We will also introduce the idea of a TensorFlow Record file. This is a binary data format that replaces the flat file image representation that we used in Chapter 3. While this method of storing data is a bit more opaque, it has many advantages over the individual-image-file system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload \n",
    "%autoreload 2\n",
    "\n",
    "# Math Stuff\n",
    "import ee, scipy.misc, random, os\n",
    "import numpy as np\n",
    "from threading import Thread\n",
    "\n",
    "# debug stuff\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "from scipy import misc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Aquisition\n",
    "\n",
    "In this chapter we will be using TFRecords (TensorFlow Records) instead of flat files. Since downloading all those images took time, though, we will reuse the images we downloaded in chapter 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CH3_DATA_DIR=\"./data/ch3\"\n",
    "CH4_DATA_DIR=\"./data/ch4\"\n",
    "\n",
    "TRAIN_TFRECORD_FILENAME = os.path.join(CH4_DATA_DIR, \"ch4_train.tfrecords\")\n",
    "TEST_TFRECORD_FILENAME = os.path.join(CH4_DATA_DIR, \"ch4_test.tfrecords\")\n",
    "    \n",
    "TRAIN_IMG_DIR=os.path.join(CH3_DATA_DIR, \"train_imgs\")\n",
    "TRAIN_IMG_LABELS=os.path.join(CH3_DATA_DIR, \"train.txt\")\n",
    "TEST_IMG_DIR=os.path.join(CH3_DATA_DIR, \"test_imgs\")\n",
    "TEST_IMG_LABELS=os.path.join(CH3_DATA_DIR, \"test.txt\")\n",
    "\n",
    "# Create the data directory if necessary\n",
    "if not os.path.exists(CH4_DATA_DIR):\n",
    "    os.makedirs(CH4_DATA_DIR)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `TFRecord` file is not random access; a reader starts at byte 1 and continues reading serially until it gets to the end of the file. When we read and write from such a file we must also tell TensorFlow how the data is stored. We do that by creating a `tf.train.Example` object for each example we encode. This object is made up of a `tf.train.Features` object that describes exactly what we will be encoding.\n",
    "\n",
    "This code was based on the example at https://indico.io/blog/tensorflow-data-inputs-part1-placeholders-protobufs-queues/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Create the TRAINING TFRecord\n",
    "#\n",
    " \n",
    "print \"Recording training data in TFRecord format...\"\n",
    "# Open a TFRecord writer\n",
    "proto_writer = tf.python_io.TFRecordWriter(TRAIN_TFRECORD_FILENAME)\n",
    "\n",
    "# Iterate over every exmaple and put it in the TFRecord\n",
    "for line in tqdm(open(TRAIN_IMG_LABELS).read().splitlines()):\n",
    "    png_path, label = line.split(',')\n",
    "    img = misc.imread(png_path).flatten()\n",
    "    \n",
    "    proto_example = tf.train.Example(\n",
    "        features=tf.train.Features( # a map of string to Feature proto objects\n",
    "            feature={\n",
    "                # A Feature contains one of either a int64_list,\n",
    "                # float_list, or bytes_list\n",
    "                'label': tf.train.Feature(\n",
    "                    int64_list=tf.train.Int64List(value=[int(label)])),\n",
    "                'image': tf.train.Feature(\n",
    "                    int64_list=tf.train.Int64List(value=img.astype(\"int64\")))\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # use the proto object to serialize the example to a string\n",
    "    serialized = proto_example.SerializeToString()\n",
    "    # write the serialized object to disk\n",
    "    proto_writer.write(serialized)\n",
    "    \n",
    "proto_writer.close()\n",
    "\n",
    "#\n",
    "# Create the TESTING TFRecord\n",
    "#\n",
    " \n",
    "print \"Recording test data in TFRecord format...\"\n",
    "# Open a TFRecord writer\n",
    "proto_writer = tf.python_io.TFRecordWriter(TEST_TFRECORD_FILENAME)\n",
    "\n",
    "# Iterate over every exmaple and put it in the TFRecord\n",
    "for line in tqdm(open(TEST_IMG_LABELS).read().splitlines()):\n",
    "    png_path, label = line.split(',')\n",
    "    img = misc.imread(png_path).flatten()\n",
    "    \n",
    "    proto_example = tf.train.Example(\n",
    "        features=tf.train.Features( # a map of string to Feature proto objects\n",
    "            feature={\n",
    "                # A Feature contains one of either a int64_list,\n",
    "                # float_list, or bytes_list\n",
    "                'label': tf.train.Feature(\n",
    "                    int64_list=tf.train.Int64List(value=[int(label)])),\n",
    "                'image': tf.train.Feature(\n",
    "                    int64_list=tf.train.Int64List(value=img.astype(\"int64\")))\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # use the proto object to serialize the example to a string\n",
    "    serialized = proto_example.SerializeToString()\n",
    "    # write the serialized object to disk\n",
    "    proto_writer.write(serialized)\n",
    "\n",
    "proto_writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Look inside the TFRecord to prove to ourselves that the records are correctly recorded.\n",
    "#\n",
    "\n",
    "i = 0\n",
    "for serialized_example in tf.python_io.tf_record_iterator(TRAIN_TFRECORD_FILENAME):\n",
    "    i = i + 1\n",
    "    if i > 3:\n",
    "        break\n",
    "    example = tf.train.Example()\n",
    "    example.ParseFromString(serialized_example)\n",
    "\n",
    "    # traverse the Example format to get data\n",
    "    image = example.features.feature['image'].int64_list.value\n",
    "    label = example.features.feature['label'].int64_list.value[0]\n",
    "\n",
    "    # display data\n",
    "    print \"Label:\", label\n",
    "    img = np.array(image).astype(\"float32\")\n",
    "    plt.imshow(img.reshape((50,50, 3)), cmap='gray'); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder Design\n",
    "\n",
    "The autoencoder design is split into 2 parts: the data pipeline and the network architecture.\n",
    "\n",
    "### Data Pipeline\n",
    "Note that we are using TensorFlow functions (tf.foobar) instead of numpy functions (np.foobar). The data pipeline will actually be part of the TensorFlow graph in this case, which will speed up our network considerably. In chapter 3 we had to go out to the filesystem and load/read hundreds of images every iteration before calculations could be started. Since TensorFlow will be in charge of loading its own data it can preload data at the same time as it is doing neural network calculations, if that is most efficient. In any case, most of TensorFlow is written in C++ and optimaized for its neural network calculations which will give us a performace boost.\n",
    "\n",
    "One more point: while we encoded the entire 3-channel image into the TFRecord, a color image will probably be too complex for our architecture to learn. Our data pipeline will only keep the red channel.\n",
    "\n",
    "### Network Architecture\n",
    "This section is pretty straightforward. Instead of the output being a 3-element long vector, it will be a 2500 element vector representing a 50x50 pixel image. One key difference is that we are not using batch normalization in this network; we want the network to recreate the exact input image, not a normalized version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Required input placeholders\n",
    "#\n",
    "training = tf.placeholder(dtype=tf.bool, name=\"is_training\") # True if training, False if testing\n",
    "batch_size = tf.placeholder(dtype=tf.int32, name=\"batch_size\")\n",
    "\n",
    "#\n",
    "# Our data-loading pipeline.\n",
    "#\n",
    "\n",
    "def get_batch(proto_filename, batch_size):\n",
    "    filename_queue = tf.train.string_input_producer([proto_filename], num_epochs=None)\n",
    "    proto_reader = tf.TFRecordReader()\n",
    "\n",
    "    # Examples from the TFRecord.\n",
    "    _, serialized_example = proto_reader.read(filename_queue)\n",
    "\n",
    "    # The serialized example is converted back to actual values\n",
    "    # by describing the format of the objects to be returned\n",
    "    features = tf.parse_single_example(\n",
    "        serialized_example,\n",
    "        features={\n",
    "            # We know the length of both fields. If not the\n",
    "            # tf.VarLenFeature could be used\n",
    "            'label': tf.FixedLenFeature([], tf.int64),\n",
    "            'image': tf.FixedLenFeature([50*50*3], tf.int64)\n",
    "        })\n",
    "\n",
    "    # now we have the raw data\n",
    "    label = features['label']\n",
    "    image = features['image']\n",
    "\n",
    "    # wrest the data into the desired orientation.\n",
    "    cast_img = tf.cast(image, tf.float32)\n",
    "    threeD_img = tf.reshape(cast_img, [50,50, 3])\n",
    "    bw_img = tf.slice(threeD_img, [0, 0, 0], [50, 50, 1])\n",
    "    oneD_img = tf.reshape(bw_img, [50*50])\n",
    "    squashed_img = tf.divide(oneD_img, 255)\n",
    "\n",
    "    # ...and batch it\n",
    "    # Since a TFRecord can only be read sequentially, in order to shuffle the\n",
    "    # data we use a tf.train.shuffle_batch() tensor. This function queues up\n",
    "    # a bunch of examples and shuffles the data based on what it has queued up.\n",
    "    # It's a good idea to know what this function is doing. Since we didn't\n",
    "    # shuffle our data before storing it in in the TFRecord, out first few passes\n",
    "    # could be at risk of being a but undershuffled. But since we don't have that many\n",
    "    # data examples everything should be pretty well shuffled after a few iterations.\n",
    "    images_batch, labels_batch = tf.train.shuffle_batch(\n",
    "        [squashed_img, label], batch_size=batch_size,\n",
    "        capacity=2000,\n",
    "        min_after_dequeue=1000)\n",
    "\n",
    "    return images_batch, labels_batch\n",
    "\n",
    "\n",
    "#\n",
    "# Load the correct training/test data by using a conditional tensor.\n",
    "#\n",
    "train_images_batch, train_labels_batch = get_batch(TRAIN_TFRECORD_FILENAME, batch_size)\n",
    "test_images_batch, test_labels_batch = get_batch(TEST_TFRECORD_FILENAME, batch_size)\n",
    "\n",
    "images_batch, labels_batch = tf.cond(training,\n",
    "                         lambda: (train_images_batch, train_labels_batch),\n",
    "                         lambda: (test_images_batch, test_labels_batch)\n",
    "                        )\n",
    "\n",
    "\n",
    "#\n",
    "# Model\n",
    "#\n",
    "x = images_batch\n",
    "y_ = images_batch\n",
    "\n",
    "\n",
    "# Ensure our images are the correct shape\n",
    "input_layer = tf.reshape(x, [-1, 50, 50, 1])\n",
    "\n",
    "\n",
    "#\n",
    "# Convolutions\n",
    "#\n",
    "\n",
    "# Convolutional Layer #1\n",
    "conv1 = tf.layers.conv2d(\n",
    "    inputs=input_layer,\n",
    "    filters=16,\n",
    "    kernel_size=[3, 3],\n",
    "    strides = [1,1],\n",
    "    padding=\"valid\",\n",
    "    activation=tf.nn.relu)\n",
    "pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
    "\n",
    "# Convolutional Layer #2\n",
    "conv2 = tf.layers.conv2d(\n",
    "    inputs=pool1,\n",
    "    filters=16,\n",
    "    kernel_size=[3, 3],\n",
    "    strides = [1,1],\n",
    "    padding=\"valid\",\n",
    "    activation=tf.nn.relu)\n",
    "pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
    "\n",
    "# Convolutional Layer #3\n",
    "conv3 = tf.layers.conv2d(\n",
    "    inputs=pool2,\n",
    "    filters=16,\n",
    "    kernel_size=[4, 4],\n",
    "    strides = [1,1],\n",
    "    padding=\"valid\",\n",
    "    activation=tf.nn.relu)\n",
    "pool3 = tf.layers.max_pooling2d(inputs=conv3, pool_size=[2, 2], strides=2)\n",
    "\n",
    "# Dense Layers\n",
    "conv3_flat = tf.reshape(pool3, [-1, 4 * 4 * 16])\n",
    "fc1 = tf.layers.dense(inputs=conv3_flat, units=256, activation=tf.nn.relu)\n",
    "\n",
    "# Here is our bottleneck; the part of our network with the fewest activations.\n",
    "fc2 = tf.layers.dense(inputs=fc1, units=256, activation=tf.nn.relu)\n",
    "\n",
    "# This layer uses information from the bottleneck to try to recreate the\n",
    "# original image.\n",
    "fc3 = tf.layers.dense(inputs=fc2, units=50*50, activation=tf.nn.relu)\n",
    "\n",
    "\n",
    "#\n",
    "# Loss\n",
    "#\n",
    "\n",
    "loss = None\n",
    "train_op = None\n",
    "y_pred = fc3\n",
    "y_true = y_\n",
    "\n",
    "# We're not using the softmax cross-entropy loss function here;\n",
    "# Instead we're going to define the loss as the mean pixel difference\n",
    "# between the input and output images.\n",
    "loss = tf.reduce_mean(tf.pow(y_true - y_pred, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Train\n",
    "#\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "starter_learning_rate = 1e-3\n",
    "learning_rate = tf.train.exponential_decay(learning_rate = starter_learning_rate,\n",
    "                                           global_step = global_step,\n",
    "                                           decay_steps = 200,\n",
    "                                           decay_rate = 0.96,\n",
    "                                           staircase=True)\n",
    "\n",
    "\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS) # batch norm\n",
    "with tf.control_dependencies(update_ops): # batch norm\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "\n",
    "# Initialize tensorflow\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# start the data reader tensors\n",
    "tf.train.start_queue_runners(sess=sess)\n",
    "\n",
    "# And run 10k iterations\n",
    "for i in range(30000):\n",
    "    \n",
    "    # Debug output\n",
    "    if i%100 == 1:\n",
    "        prediction, loss_out = sess.run( fetches = [y_pred, loss],\n",
    "                                         feed_dict={training: False, batch_size:256})\n",
    "        print \"Step:\", i, \", Loss:\", loss_out\n",
    "    \n",
    "    # run an iteration\n",
    "    optimizer.run(feed_dict={training: True, batch_size:256})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "\n",
    "Remember, an autoencoder attempts to recreate the original image. In the next cell we can feed an image into the autoencoder and see what image the autoencoder is able to recreate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "\n",
    "# farm, city, mountain\n",
    "\n",
    "\n",
    "img_in, prediction, loss_out, feature_vector = sess.run( fetches = [y_true,y_pred, loss, fc2],\n",
    "                                         feed_dict={training: False, batch_size:1})\n",
    "\n",
    "img_in = img_in.reshape((50,50))\n",
    "prediction = prediction.reshape((50,50))\n",
    "\n",
    "print \"Prediction difference:\", np.mean(np.square(img_in - prediction))\n",
    "\n",
    "print \"Original Image:\"\n",
    "plt.imshow(img_in, cmap='gray', vmin=0, vmax=1); plt.show()\n",
    "\n",
    "print \"Autoencoder-recreated Image:\"\n",
    "plt.imshow(prediction, cmap='gray', vmin=0, vmax=1); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "We can use the bottleneck activations as a \"feature vector\" for an image. We can feed these activations into a clustering algorithm to see if we can group similar images together.\n",
    "\n",
    "The next section uses K-means clusetering to group images together, using activations from the `fc2` layer of the autoencoder. Since K-means requires that we know 'K' ahead of time (and we don't know it), we will run the algorithm several times with different values of K and measuer the average distance each data point falls from its cluseter center. We can try to find some sort of 'elbow' where we start getting diminishing returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Clustering\n",
    "#\n",
    "\n",
    "from pylab import plot,show\n",
    "from numpy import vstack,array\n",
    "from numpy.random import rand\n",
    "from scipy.cluster.vq import kmeans,vq\n",
    "from scipy.spatial import distance\n",
    "\n",
    "def featurize_image():\n",
    "    img, feature_vector = sess.run( fetches = [y_true, fc2], feed_dict={training: False, batch_size:1})\n",
    "    return img, feature_vector\n",
    "\n",
    "\n",
    "#\n",
    "# Calculate Error\n",
    "#\n",
    "def cluster_error(data, centroids, idx):\n",
    "    sum_error = 0\n",
    "    for i in range(idx.shape[0]):\n",
    "        error = dst = distance.euclidean(data[i, :], centroids[idx[i], :])\n",
    "        sum_error += error\n",
    "    return sum_error\n",
    "        \n",
    "        \n",
    "#\n",
    "# Collect Data\n",
    "#\n",
    "data_img_list = []\n",
    "\n",
    "for i in range(500):\n",
    "    img, feature_vector = featurize_image()\n",
    "    sample = {\n",
    "        'img': img,\n",
    "        'feature_vector': feature_vector\n",
    "    }\n",
    "    data_img_list.append(sample)\n",
    "\n",
    "data = np.squeeze(np.array([d['feature_vector'] for d in data_img_list]))\n",
    "   \n",
    "\n",
    "\n",
    "    \n",
    "#\n",
    "# K Means\n",
    "#\n",
    "\n",
    "k_list = []\n",
    "error_list = []\n",
    "\n",
    "for k in range(2,20):\n",
    "    centroids,_ = kmeans(data,k)\n",
    "    # assign each sample to a cluster\n",
    "    idx,_ = vq(data,centroids)\n",
    "\n",
    "    error = cluster_error(data, centroids, idx)\n",
    "    print \"k=\", k, \":\", error\n",
    "    k_list.append(k)\n",
    "    error_list.append(cluster_error(data, centroids, idx))\n",
    "    \n",
    "plt.plot(k_list, error_list, '-o')\n",
    "plt.show()\n",
    "    \n",
    "    \n",
    "# In my experiments it seemed like K=5 gave the best results. But try\n",
    "# different values and see what you get!\n",
    "K=5\n",
    "\n",
    "centroids,_ = kmeans(data,K)\n",
    "\n",
    "# assign each sample to a cluster\n",
    "idx,_ = vq(data,centroids)\n",
    "\n",
    "\n",
    "\n",
    "for k in range(K):\n",
    "    p = 1\n",
    "    print \"Cluster\", k\n",
    "    \n",
    "    # collect images in class\n",
    "    class_images = [data_img_list[i]\n",
    "                    for i in range(idx.shape[0])\n",
    "                    if idx[i] == k]\n",
    "    \n",
    "    # sort the image list based on each image's distance from the cluster center\n",
    "    class_images.sort(\n",
    "        key=lambda i_data: distance.euclidean(i_data['feature_vector'], centroids[k])\n",
    "    )\n",
    "    \n",
    "    for image_data in class_images:\n",
    "        if p <= 4*5:\n",
    "            plt.subplot(4, 5, p)\n",
    "            plt.axis('off')    \n",
    "            plt.imshow(image_data['img'].reshape((50,50)), cmap='gray')\n",
    "            p += 1\n",
    "    plt.show()\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
